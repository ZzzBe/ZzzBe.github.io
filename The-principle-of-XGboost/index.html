<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang="en">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.3" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.3">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.3">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.3">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.3" color="#222">





  <meta name="keywords" content="Deep Learning," />










<meta name="description" content="Xgboost Introduction to XGboostXGBoost is short for “Extreme Gradient Boosting”, This is a tutorial on gradient boosted trees by the author of xgboost.XGBoost is used for supervised learning problems,">
<meta name="keywords" content="Deep Learning">
<meta property="og:type" content="article">
<meta property="og:title" content="The principle of XGboost">
<meta property="og:url" content="https://ZzzBe.github.io/The-principle-of-XGboost/index.html">
<meta property="og:site_name" content="Jack&#39;s Notebook">
<meta property="og:description" content="Xgboost Introduction to XGboostXGBoost is short for “Extreme Gradient Boosting”, This is a tutorial on gradient boosted trees by the author of xgboost.XGBoost is used for supervised learning problems,">
<meta property="og:locale" content="en">
<meta property="og:image" content="https://zzzbe.github.io/The-principle-of-XGboost/cart.png">
<meta property="og:image" content="https://zzzbe.github.io/The-principle-of-XGboost/twocart.png">
<meta property="og:image" content="https://zzzbe.github.io/The-principle-of-XGboost/fg.png">
<meta property="og:image" content="https://zzzbe.github.io/The-principle-of-XGboost/struct.png">
<meta property="og:image" content="https://zzzbe.github.io/The-principle-of-XGboost/split_find.png">
<meta property="og:updated_time" content="2017-10-09T14:54:33.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="The principle of XGboost">
<meta name="twitter:description" content="Xgboost Introduction to XGboostXGBoost is short for “Extreme Gradient Boosting”, This is a tutorial on gradient boosted trees by the author of xgboost.XGBoost is used for supervised learning problems,">
<meta name="twitter:image" content="https://zzzbe.github.io/The-principle-of-XGboost/cart.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '5.1.3',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://ZzzBe.github.io/The-principle-of-XGboost/"/>





  <title>The principle of XGboost | Jack's Notebook</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Jack's Notebook</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">In me the tiger sniffs the rose.</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/resume.html" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://ZzzBe.github.io/The-principle-of-XGboost/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="ZHANG Bo">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jack's Notebook">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">The principle of XGboost</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-09-20T23:22:12+08:00">
                2017-09-20
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p><strong><em>Xgboost</em></strong></p>
<h1 id="Introduction-to-XGboost"><a href="#Introduction-to-XGboost" class="headerlink" title="Introduction to XGboost"></a>Introduction to XGboost</h1><p>XGBoost is short for “Extreme Gradient Boosting”, This is a <a href="https://homes.cs.washington.edu/~tqchen/pdf/BoostedTree.pdf" target="_blank" rel="external">tutorial</a> on gradient boosted trees by the author of xgboost.XGBoost is used for supervised learning problems, where we use the training data (with multiple features) $x_i$ to predict a target variable $y_i$.</p>
<a id="more"></a>
<h1 id="Tree-Ensemble"><a href="#Tree-Ensemble" class="headerlink" title="Tree Ensemble"></a>Tree Ensemble</h1><p>To begin with, let’s first learn about the model of xgboost: <strong>tree ensembles</strong>. </p>
<h2 id="Introduction-to-CART"><a href="#Introduction-to-CART" class="headerlink" title="Introduction to CART"></a>Introduction to CART</h2><p><a href="https://en.wikipedia.org/wiki/Decision_tree_learning" target="_blank" rel="external">Decision tree learning</a> uses a <strong><em>decision tree</em></strong> (as a predictive model) to go from observations about an item (represented in the branches) to conclusions about the item’s target value (represented in the leaves). Tree models where the target variable can take a discrete set of values are called <strong><em>classification trees</em></strong>; in these tree structures, leaves represent class labels and branches represent conjunctions of features that lead to those class labels.Decision trees where the target variable can take continuous values (typically real numbers) are called <strong><em>regression trees</em></strong>.</p>
<p><strong>Decision trees used in data mining are of two main types:</strong></p>
<ul>
<li>Classification tree analysis is when the predicted outcome is the class to which the data belongs.</li>
<li>Regression tree analysis is when the predicted outcome can be considered a real number.</li>
</ul>
<p>The tree ensemble model is a set of classification and regression trees (<a href="https://en.wikipedia.org/wiki/Predictive_analytics#Classification_and_regression_trees_.28CART.29" target="_blank" rel="external">CART</a>). <strong><em>CART</em></strong> are a non-parametric decision tree learning technique that produces either classification or regression trees, depending on whether the dependent variable is categorical or numeric, respectively.</p>
<h2 id="A-simple-example"><a href="#A-simple-example" class="headerlink" title="A simple example"></a>A simple example</h2><p>Here’s a simple example of a CART that classifies whether someone will like computer games.</p>
<p><img src="/The-principle-of-XGboost/cart.png" alt=""></p>
<h2 id="CART-Model"><a href="#CART-Model" class="headerlink" title="CART Model"></a>CART Model</h2><p>Usually, a single tree is not strong enough to be used in practice. What is actually used is the so-called tree ensemble model, which sums the prediction of multiple trees together.</p>
<p><img src="/The-principle-of-XGboost/twocart.png" alt=""></p>
<p>The prediction scores of each individual tree are summed up to get the final score.</p>
<h1 id="Tree-Boosting"><a href="#Tree-Boosting" class="headerlink" title="Tree Boosting"></a>Tree Boosting</h1><p>Assuming we have $K$ trees,mathematically, we can write our model in the form:</p>
<p>$$ \hat{y<em>i} = \sum</em>{k=1}^K f_k(x_i), f_k \in \mathcal{F} $$</p>
<p>where $K$ is the number of trees, $f$ is a function in the functional space  $\mathcal{F}$, and  $\mathcal{F}$ is the set of all possible CARTs. Therefore our objective to optimize can be written as</p>
<p>$$ \text{obj}(\theta) = \sum_i^n l(y_i, \hat{y<em>i}) + \sum</em>{k=1}^K \Omega(f_k) $$</p>
<p>The first part is the value of training loss,the second part is the value of the complexity of the trees.</p>
<h2 id="Additive-Training"><a href="#Additive-Training" class="headerlink" title="Additive Training"></a>Additive Training</h2><p>we use an additive strategy: fix what we have learned, and add one new tree at a time. We write the prediction value at step $t$ as $\hat{y_i}^t$.So we have:</p>
<p>$$ \hat{y_i}^0 = 0 $$<br>$$ \hat{y_i}^1 = \hat{y_i}^0 + f_1(x_i)$$<br>$$…$$<br>$$\hat{y_i}^t = \hat{y_i}^{t+1} + f_t(x<em>i) = \sum</em>{k=1}^t f_k(x_i)$$</p>
<p>If we consider using MSE as our loss function, it becomes the following form.</p>
<p>$$\begin{split}\text{obj}^{t} &amp; = \sum_{i=1}^n (y_i - (\hat{y_i}^{t-1} + f_t(x<em>i)))^2 + \sum</em>{i=1}^t\Omega(f<em>i) \ &amp; = \sum</em>{i=1}^n [2(\hat{y_i}^{t-1} - y_i)f_t(x_i) + f_t(x_i)^2] + \Omega(f_t) + constant \end{split}$$</p>
<h2 id="Optimize-the-objective"><a href="#Optimize-the-objective" class="headerlink" title="Optimize the objective"></a>Optimize the objective</h2><p>The form of MSE is friendly, with a first order term (usually called the residual) and a quadratic term. For other losses of interest (for example, logistic loss), it is not so easy to get such a nice form. So in the general case, we take the Taylor expansion of the loss function up to the second order</p>
<p>$$ \text{obj}^{(t)} = \sum_{i=1}^n [l(y_i, {\hat{y_i}^{t-1} +g_i f_t(x_i) +\frac{1}{2} h_i f_t^2(x_i)] + \Omega(f_t) + constant$$</p>
<p>where the $g_i$ and $h_i$ are defined as:</p>
<p>$$g<em>i = \partial</em>{\hat{y_i}^{t-1}} l(y_i, {\hat{y_i}}^{t-1})$$<br>$$h<em>i = \partial</em>{\hat{y_i}^{t-1}}^2 l(y_i, {\hat{y_i}}^{t-1})$$</p>
<p>After we remove all the constants, the specific objective at step $t$ becomes</p>
<p>$$ \sum_{i=1}^n [g_i f_t(x_i) + \frac{1}{2} h_i f_t^2(x_i)] + \Omega(f_t) $$<br>This becomes our optimization goal for the new tree. One important advantage of this definition is that it only depends on $g_i$ and $h_i$ . This is how xgboost can support custom loss functions. We can optimize every loss function, including logistic regression and weighted logistic regression, using exactly the same solver that takes $ g_i$ and $h_i$ as input!</p>
<h2 id="Refine-the-definition-of-tree"><a href="#Refine-the-definition-of-tree" class="headerlink" title="Refine the definition of tree"></a>Refine the definition of tree</h2><p>We have introduced the training step, but wait, there is one important thing, the regularization! We need to define the complexity of the tree $\Omega(f)$. In order to do so, let us first refine the definition of the tree $f(x) $ as</p>
<p>$$f<em>t(x) = w</em>{q(x)}, w \in R^T, q:R^d\rightarrow {1,2,\cdots,T} $$<br>Here $ w $ is the vector of scores on leaves, $ q $ is a function assigning each data point to the corresponding leaf, and $ T $ is the number of leaves</p>
<p><img src="/The-principle-of-XGboost/fg.png" alt=""></p>
<h2 id="Define-the-Complexity-of-Tree"><a href="#Define-the-Complexity-of-Tree" class="headerlink" title="Define the Complexity of Tree"></a>Define the Complexity of Tree</h2><p>In XGBoost, we define the complexity as</p>
<p>$$\Omega(f) = \gamma T + \frac{1}{2}\lambda \sum_{j=1}^T w_j^2 $$</p>
<h2 id="The-Structure-Score"><a href="#The-Structure-Score" class="headerlink" title="The Structure Score"></a>The Structure Score</h2><p>Here is the magical part of the derivation. After reformalizing the tree model, we can write the objective value with the $ t$-th tree as:</p>
<p>$$\begin{split}Obj^{(t)} &amp;\approx \sum_{i=1}^n [g<em>i w</em>{q(x_i)} + \frac{1}{2} h<em>i w</em>{q(x<em>i)}^2] + \gamma T + \frac{1}{2}\lambda \sum</em>{j=1}^T w<em>j^2\ &amp;= \sum^T</em>{j=1} [(\sum_{i\in I_j} g_i) w<em>j + \frac{1}{2} (\sum</em>{i\in I_j} h_i + \lambda) w_j^2 ] + \gamma T \end{split}$$<br>where $I_j = {i|q(x_i)=j} $ is the set of indices of data points assigned to the $ j $-th leaf. Notice that in the second line we have changed the index of the summation because all the data points on the same leaf get the same score. We could further compress the expression by defining $ G<em>j = \sum</em>{i\in I_j} g_i $ and $ H<em>j = \sum</em>{i\in I_j} h_i $:</p>
<p>$$\text{obj}^{(t)} = \sum^T_{j=1} [G_jw_j + \frac{1}{2} (H_j+\lambda) w_j^2] +\gamma T $$<br>In this equation $ w_j $ are independent with respect to each other, the form $$ G_jw_j+\frac{1}{2}(H_j+\lambda)w_j^2 $$ is quadratic and the best $w_j $ for a given structure $q(x)$ and the best objective reduction we can get is:</p>
<p>$$\begin{split}w_j^\ast = -\frac{G_j}{H<em>j+\lambda}\ \text{obj}^\ast = -\frac{1}{2} \sum</em>{j=1}^T \frac{G_j^2}{H_j+\lambda} + \gamma T \end{split}$$<br>The last equation measures how good a tree structure $q(x)$ is.</p>
<p><img src="/The-principle-of-XGboost/struct.png" alt=""></p>
<h2 id="Learn-the-tree-structure"><a href="#Learn-the-tree-structure" class="headerlink" title="Learn the tree structure"></a>Learn the tree structure</h2><p>Now that we have a way to measure how good a tree is, ideally we would enumerate all possible trees and pick the best one. In practice this is intractable, so we will try to optimize one level of the tree at a time. Specifically we try to split a leaf into two leaves, and the score it gains is</p>
<p>$$ Gain = \frac{1}{2} \left[\frac{G_L^2}{H_L+\lambda}+\frac{G_R^2}{H_R+\lambda}-\frac{(G_L+G_R)^2}{H_L+H_R+\lambda}\right] - \gamma $$<br>This formula can be decomposed as:</p>
<ul>
<li>the score on the new left leaf </li>
<li>the score on the new right leaf </li>
<li>the score on the original leaf </li>
<li>regularization on the additional leaf </li>
</ul>
<p>For real valued data, we usually want to search for an optimal split. To efficiently do so, we place all the instances in sorted order, like the following picture. </p>
<p><img src="/The-principle-of-XGboost/split_find.png" alt=""></p>
<p>A left to right scan is sufficient to calculate the structure score of all possible split solutions, and we can find the best split efficiently.</p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Deep-Learning/" rel="tag"># Deep Learning</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/How-to-implement-a-neural-network/" rel="next" title="How to implement a neural network">
                <i class="fa fa-chevron-left"></i> How to implement a neural network
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">ZHANG Bo</p>
              <p class="site-description motion-element" itemprop="description">Stay hungry.Stay foolish.</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">3</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">2</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          <div class="links-of-author motion-element">
            
          </div>

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Introduction-to-XGboost"><span class="nav-number">1.</span> <span class="nav-text">Introduction to XGboost</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Tree-Ensemble"><span class="nav-number">2.</span> <span class="nav-text">Tree Ensemble</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Introduction-to-CART"><span class="nav-number">2.1.</span> <span class="nav-text">Introduction to CART</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#A-simple-example"><span class="nav-number">2.2.</span> <span class="nav-text">A simple example</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#CART-Model"><span class="nav-number">2.3.</span> <span class="nav-text">CART Model</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Tree-Boosting"><span class="nav-number">3.</span> <span class="nav-text">Tree Boosting</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Additive-Training"><span class="nav-number">3.1.</span> <span class="nav-text">Additive Training</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Optimize-the-objective"><span class="nav-number">3.2.</span> <span class="nav-text">Optimize the objective</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Refine-the-definition-of-tree"><span class="nav-number">3.3.</span> <span class="nav-text">Refine the definition of tree</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Define-the-Complexity-of-Tree"><span class="nav-number">3.4.</span> <span class="nav-text">Define the Complexity of Tree</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#The-Structure-Score"><span class="nav-number">3.5.</span> <span class="nav-text">The Structure Score</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Learn-the-tree-structure"><span class="nav-number">3.6.</span> <span class="nav-text">Learn the tree structure</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">ZHANG Bo</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a> v5.1.3</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.3"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.3"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.3"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.3"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.3"></script>



  


  




	





  





  












  





  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  


  <!-- 背景动画 -->
  <script type="text/javascript" src="/js/src/particle.js"></script>
</body>
</html>
