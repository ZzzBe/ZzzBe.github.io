<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang="en">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.3" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.3">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.3">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.3">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.3" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta name="description" content="Stay hungry.Stay foolish.">
<meta property="og:type" content="website">
<meta property="og:title" content="Jack&#39;s Notebook">
<meta property="og:url" content="https://ZzzBe.github.io/index.html">
<meta property="og:site_name" content="Jack&#39;s Notebook">
<meta property="og:description" content="Stay hungry.Stay foolish.">
<meta property="og:locale" content="en">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Jack&#39;s Notebook">
<meta name="twitter:description" content="Stay hungry.Stay foolish.">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '5.1.3',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://ZzzBe.github.io/"/>





  <title>Jack's Notebook</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Jack's Notebook</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">In me the tiger sniffs the rose.</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/resume.html" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://ZzzBe.github.io/The-principle-of-XGboost/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="ZHANG Bo">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jack's Notebook">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/The-principle-of-XGboost/" itemprop="url">The principle of XGboost</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-09-20T23:22:12+08:00">
                2017-09-20
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><strong><em>Xgboost</em></strong></p>
<h1 id="Introduction-to-XGboost"><a href="#Introduction-to-XGboost" class="headerlink" title="Introduction to XGboost"></a>Introduction to XGboost</h1><p>XGBoost is short for “Extreme Gradient Boosting”, This is a <a href="https://homes.cs.washington.edu/~tqchen/pdf/BoostedTree.pdf" target="_blank" rel="external">tutorial</a> on gradient boosted trees by the author of xgboost.XGBoost is used for supervised learning problems, where we use the training data (with multiple features) $x_i$ to predict a target variable $y_i$.</p>
<h1 id="Tree-Ensemble"><a href="#Tree-Ensemble" class="headerlink" title="Tree Ensemble"></a>Tree Ensemble</h1><p>To begin with, let’s first learn about the model of xgboost: <strong>tree ensembles</strong>. </p>
<h2 id="Introduction-to-CART"><a href="#Introduction-to-CART" class="headerlink" title="Introduction to CART"></a>Introduction to CART</h2><p><a href="https://en.wikipedia.org/wiki/Decision_tree_learning" target="_blank" rel="external">Decision tree learning</a> uses a <strong><em>decision tree</em></strong> (as a predictive model) to go from observations about an item (represented in the branches) to conclusions about the item’s target value (represented in the leaves). Tree models where the target variable can take a discrete set of values are called <strong><em>classification trees</em></strong>; in these tree structures, leaves represent class labels and branches represent conjunctions of features that lead to those class labels.Decision trees where the target variable can take continuous values (typically real numbers) are called <strong><em>regression trees</em></strong>.</p>
<p><strong>Decision trees used in data mining are of two main types:</strong></p>
<ul>
<li>Classification tree analysis is when the predicted outcome is the class to which the data belongs.</li>
<li>Regression tree analysis is when the predicted outcome can be considered a real number.</li>
</ul>
<p>The tree ensemble model is a set of classification and regression trees (<a href="https://en.wikipedia.org/wiki/Predictive_analytics#Classification_and_regression_trees_.28CART.29" target="_blank" rel="external">CART</a>). <strong><em>CART</em></strong> are a non-parametric decision tree learning technique that produces either classification or regression trees, depending on whether the dependent variable is categorical or numeric, respectively.</p>
<h2 id="A-simple-example"><a href="#A-simple-example" class="headerlink" title="A simple example"></a>A simple example</h2><p>Here’s a simple example of a CART that classifies whether someone will like computer games.</p>
<p><img src="The-principle-of-XGboost/cart.png" alt=""></p>
<h2 id="CART-Model"><a href="#CART-Model" class="headerlink" title="CART Model"></a>CART Model</h2><p>Usually, a single tree is not strong enough to be used in practice. What is actually used is the so-called tree ensemble model, which sums the prediction of multiple trees together.</p>
<p><img src="The-principle-of-XGboost/twocart.png" alt=""></p>
<p>The prediction scores of each individual tree are summed up to get the final score.</p>
<h1 id="Tree-Boosting"><a href="#Tree-Boosting" class="headerlink" title="Tree Boosting"></a>Tree Boosting</h1><p>Assuming we have $K$ trees,mathematically, we can write our model in the form:</p>
<p>$$ \hat{y<em>i} = \sum</em>{k=1}^K f_k(x_i), f_k \in \mathcal{F} $$</p>
<p>where $K$ is the number of trees, $f$ is a function in the functional space  $\mathcal{F}$, and  $\mathcal{F}$ is the set of all possible CARTs. Therefore our objective to optimize can be written as</p>
<p>$$ \text{obj}(\theta) = \sum_i^n l(y_i, \hat{y<em>i}) + \sum</em>{k=1}^K \Omega(f_k) $$</p>
<p>The first part is the value of training loss,the second part is the value of the complexity of the trees.</p>
<h2 id="Additive-Training"><a href="#Additive-Training" class="headerlink" title="Additive Training"></a>Additive Training</h2><p>we use an additive strategy: fix what we have learned, and add one new tree at a time. We write the prediction value at step $t$ as $\hat{y_i}^t$.So we have:</p>
<p>$$ \hat{y_i}^0 = 0 $$<br>$$ \hat{y_i}^1 = \hat{y_i}^0 + f_1(x_i)$$<br>$$…$$<br>$$\hat{y_i}^t = \hat{y_i}^{t+1} + f_t(x<em>i) = \sum</em>{k=1}^t f_k(x_i)$$</p>
<p>If we consider using MSE as our loss function, it becomes the following form.</p>
<p>$$\begin{split}\text{obj}^{t} &amp; = \sum_{i=1}^n (y_i - (\hat{y_i}^{t-1} + f_t(x<em>i)))^2 + \sum</em>{i=1}^t\Omega(f<em>i) \ &amp; = \sum</em>{i=1}^n [2(\hat{y_i}^{t-1} - y_i)f_t(x_i) + f_t(x_i)^2] + \Omega(f_t) + constant \end{split}$$</p>
<h2 id="Optimize-the-objective"><a href="#Optimize-the-objective" class="headerlink" title="Optimize the objective"></a>Optimize the objective</h2><p>The form of MSE is friendly, with a first order term (usually called the residual) and a quadratic term. For other losses of interest (for example, logistic loss), it is not so easy to get such a nice form. So in the general case, we take the Taylor expansion of the loss function up to the second order</p>
<p>$$ \text{obj}^{(t)} = \sum_{i=1}^n [l(y_i, {\hat{y_i}^{t-1} +g_i f_t(x_i) +\frac{1}{2} h_i f_t^2(x_i)] + \Omega(f_t) + constant$$</p>
<p>where the $g_i$ and $h_i$ are defined as:</p>
<p>$$g<em>i = \partial</em>{\hat{y_i}^{t-1}} l(y_i, {\hat{y_i}}^{t-1})$$<br>$$h<em>i = \partial</em>{\hat{y_i}^{t-1}}^2 l(y_i, {\hat{y_i}}^{t-1})$$</p>
<p>After we remove all the constants, the specific objective at step $t$ becomes</p>
<p>$$ \sum_{i=1}^n [g_i f_t(x_i) + \frac{1}{2} h_i f_t^2(x_i)] + \Omega(f_t) $$<br>This becomes our optimization goal for the new tree. One important advantage of this definition is that it only depends on $g_i$ and $h_i$ . This is how xgboost can support custom loss functions. We can optimize every loss function, including logistic regression and weighted logistic regression, using exactly the same solver that takes $ g_i$ and $h_i$ as input!</p>
<h2 id="Refine-the-definition-of-tree"><a href="#Refine-the-definition-of-tree" class="headerlink" title="Refine the definition of tree"></a>Refine the definition of tree</h2><p>We have introduced the training step, but wait, there is one important thing, the regularization! We need to define the complexity of the tree $\Omega(f)$. In order to do so, let us first refine the definition of the tree $f(x) $ as</p>
<p>$$f<em>t(x) = w</em>{q(x)}, w \in R^T, q:R^d\rightarrow {1,2,\cdots,T} $$<br>Here $ w $ is the vector of scores on leaves, $ q $ is a function assigning each data point to the corresponding leaf, and $ T $ is the number of leaves</p>
<p><img src="The-principle-of-XGboost/fg.png" alt=""></p>
<h2 id="Define-the-Complexity-of-Tree"><a href="#Define-the-Complexity-of-Tree" class="headerlink" title="Define the Complexity of Tree"></a>Define the Complexity of Tree</h2><p>In XGBoost, we define the complexity as</p>
<p>$$\Omega(f) = \gamma T + \frac{1}{2}\lambda \sum_{j=1}^T w_j^2 $$</p>
<h2 id="The-Structure-Score"><a href="#The-Structure-Score" class="headerlink" title="The Structure Score"></a>The Structure Score</h2><p>Here is the magical part of the derivation. After reformalizing the tree model, we can write the objective value with the $ t$-th tree as:</p>
<p>$$\begin{split}Obj^{(t)} &amp;\approx \sum_{i=1}^n [g<em>i w</em>{q(x_i)} + \frac{1}{2} h<em>i w</em>{q(x<em>i)}^2] + \gamma T + \frac{1}{2}\lambda \sum</em>{j=1}^T w<em>j^2\ &amp;= \sum^T</em>{j=1} [(\sum_{i\in I_j} g_i) w<em>j + \frac{1}{2} (\sum</em>{i\in I_j} h_i + \lambda) w_j^2 ] + \gamma T \end{split}$$<br>where $I_j = {i|q(x_i)=j} $ is the set of indices of data points assigned to the $ j $-th leaf. Notice that in the second line we have changed the index of the summation because all the data points on the same leaf get the same score. We could further compress the expression by defining $ G<em>j = \sum</em>{i\in I_j} g_i $ and $ H<em>j = \sum</em>{i\in I_j} h_i $:</p>
<p>$$\text{obj}^{(t)} = \sum^T_{j=1} [G_jw_j + \frac{1}{2} (H_j+\lambda) w_j^2] +\gamma T $$<br>In this equation $ w_j $ are independent with respect to each other, the form $$ G_jw_j+\frac{1}{2}(H_j+\lambda)w_j^2 $$ is quadratic and the best $w_j $ for a given structure $q(x)$ and the best objective reduction we can get is:</p>
<p>$$\begin{split}w_j^\ast = -\frac{G_j}{H<em>j+\lambda}\ \text{obj}^\ast = -\frac{1}{2} \sum</em>{j=1}^T \frac{G_j^2}{H_j+\lambda} + \gamma T \end{split}$$<br>The last equation measures how good a tree structure $q(x)$ is.</p>
<p><img src="The-principle-of-XGboost/struct.png" alt=""></p>
<h2 id="Learn-the-tree-structure"><a href="#Learn-the-tree-structure" class="headerlink" title="Learn the tree structure"></a>Learn the tree structure</h2><p>Now that we have a way to measure how good a tree is, ideally we would enumerate all possible trees and pick the best one. In practice this is intractable, so we will try to optimize one level of the tree at a time. Specifically we try to split a leaf into two leaves, and the score it gains is</p>
<p>$$ Gain = \frac{1}{2} \left[\frac{G_L^2}{H_L+\lambda}+\frac{G_R^2}{H_R+\lambda}-\frac{(G_L+G_R)^2}{H_L+H_R+\lambda}\right] - \gamma $$<br>This formula can be decomposed as:</p>
<ul>
<li>the score on the new left leaf </li>
<li>the score on the new right leaf </li>
<li>the score on the original leaf </li>
<li>regularization on the additional leaf </li>
</ul>
<p>For real valued data, we usually want to search for an optimal split. To efficiently do so, we place all the instances in sorted order, like the following picture. </p>
<p><img src="The-principle-of-XGboost/split_find.png" alt=""></p>
<p>A left to right scan is sufficient to calculate the structure score of all possible split solutions, and we can find the best split efficiently.</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://ZzzBe.github.io/How-to-implement-a-neural-network/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="ZHANG Bo">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jack's Notebook">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/How-to-implement-a-neural-network/" itemprop="url">How to implement a neural network</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-09-19T19:32:52+08:00">
                2017-09-19
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><strong><em> Neural Network Basics </em></strong></p>
<h3 id="Derive-the-gradients-of-the-sigmoid-function"><a href="#Derive-the-gradients-of-the-sigmoid-function" class="headerlink" title="Derive the gradients of the sigmoid function."></a>Derive the gradients of the sigmoid function.</h3><p>So,the sigmoid function is:</p>
<p>$$sigmiod(x) = \sigma (x)$$ </p>
<p>$$ \sigma ‘(x) = - \frac{1}{(1+e^{-x})^2} \cdot -e^{-x} $$<br>$$ =  \frac{1}{1+e^{-x}} \cdot \frac{e^{-x}}{1+e^{-x}} $$<br>$$ =  \frac{1}{1+e^{-x}} \cdot (1 - \frac{1}{1+e^{-x}}) $$<br>$$ =  \sigma(x) \cdot (1-\sigma(x)) $$</p>
<h3 id="Derive-the-gradient-of-the-cross-entropy-function-with-the-softmax-fuction"><a href="#Derive-the-gradient-of-the-cross-entropy-function-with-the-softmax-fuction" class="headerlink" title="Derive the gradient of the cross entropy function with the softmax fuction."></a>Derive the gradient of the cross entropy function with the softmax fuction.</h3><p>$$ CE(y,\hat{y}) = - \sum_i y_i log(\hat{y_i}) $$</p>
<p><img src="How-to-implement-a-neural-network/CEfunction_1.png" alt=""><br><img src="How-to-implement-a-neural-network/CEfunction_2.png" alt=""></p>
<h3 id="Derive-the-gradients-with-respect-to-the-inputs-x-to-an-one-hidden-layer-neural-network"><a href="#Derive-the-gradients-with-respect-to-the-inputs-x-to-an-one-hidden-layer-neural-network" class="headerlink" title="Derive the gradients with respect to the inputs x to an one-hidden-layer neural network."></a>Derive the gradients with respect to the inputs x to an one-hidden-layer neural network.</h3><p><img src="How-to-implement-a-neural-network/NN.png" alt=""><br><img src="How-to-implement-a-neural-network/GradientsWithNN.png" alt=""></p>
<h3 id="Calculate-the-value-of-parameters-in-this-neural-network"><a href="#Calculate-the-value-of-parameters-in-this-neural-network" class="headerlink" title="Calculate the value of parameters in this neural network:"></a>Calculate the value of parameters in this neural network:</h3><p>assuming the input is <em>$D_x$</em>-dimensional,the output is <em>$D_y$</em>-dimensional, and there are <em>H</em> hidden units?</p>
<p>Similar to the neural network in c:<br><img src="How-to-implement-a-neural-network/NN.png" alt=""></p>
<p>The anwser is:<br>(Dx+1)<em>H+(H+1)</em>Dy</p>
<h3 id="Fill-in-the-implementation-for-the-sigmoid-activation-function-and-its-gradient"><a href="#Fill-in-the-implementation-for-the-sigmoid-activation-function-and-its-gradient" class="headerlink" title="Fill in the implementation for the sigmoid activation function and its gradient"></a>Fill in the implementation for the sigmoid activation function and its gradient</h3><p>From the mathematical point of view,The sigmoid function should do it:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(x)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line"><span class="string">    Arguments:</span></div><div class="line"><span class="string">    x -- A scalar or numpy array.</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    Return:</span></div><div class="line"><span class="string">    s -- sigmoid(x)</span></div><div class="line"><span class="string">    """</span></div><div class="line">    s = <span class="number">1.0</span>/(<span class="number">1</span>+np.exp(-x))</div><div class="line">    <span class="comment">#raise NotImplementedError</span></div><div class="line">    </div><div class="line">    <span class="keyword">return</span> s</div></pre></td></tr></table></figure>
<p>And then,my improvement of the gradient for the sigmoid function :</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid_grad</span><span class="params">(s)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line"><span class="string">    Arguments:</span></div><div class="line"><span class="string">    s -- A scalar or numpy array.</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    Return:</span></div><div class="line"><span class="string">    ds -- Your computed gradient.</span></div><div class="line"><span class="string">    """</span></div><div class="line"></div><div class="line">    ds = s*(<span class="number">1.0</span>-s)</div><div class="line">    <span class="comment">#raise NotImplementedError</span></div><div class="line"></div><div class="line">    <span class="keyword">return</span> ds</div></pre></td></tr></table></figure>
<p>Now you can test iy by calling:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_sigmoid_basic</span><span class="params">()</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line"><span class="string">    Some simple tests to get you started.</span></div><div class="line"><span class="string">    Warning: these are not exhaustive.</span></div><div class="line"><span class="string">    """</span></div><div class="line">    <span class="keyword">print</span> <span class="string">"Running basic tests..."</span></div><div class="line">    x = np.array([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">-1</span>, <span class="number">-2</span>]])</div><div class="line">    f = sigmoid(x)</div><div class="line">    g = sigmoid_grad(f)</div><div class="line">    <span class="keyword">print</span> f</div><div class="line">    f_ans = np.array([</div><div class="line">        [<span class="number">0.73105858</span>, <span class="number">0.88079708</span>],</div><div class="line">        [<span class="number">0.26894142</span>, <span class="number">0.11920292</span>]])</div><div class="line">    <span class="keyword">assert</span> np.allclose(f, f_ans, rtol=<span class="number">1e-05</span>, atol=<span class="number">1e-06</span>)</div><div class="line">    <span class="keyword">print</span> g</div><div class="line">    g_ans = np.array([</div><div class="line">        [<span class="number">0.19661193</span>, <span class="number">0.10499359</span>],</div><div class="line">        [<span class="number">0.19661193</span>, <span class="number">0.10499359</span>]])</div><div class="line">    <span class="keyword">assert</span> np.allclose(g, g_ans, rtol=<span class="number">1e-05</span>, atol=<span class="number">1e-06</span>)</div><div class="line"></div><div class="line">test_sigmoid_basic()</div></pre></td></tr></table></figure>
<pre><code>Running basic tests...
[[ 0.73105858  0.88079708]
 [ 0.26894142  0.11920292]]
[[ 0.19661193  0.10499359]
 [ 0.19661193  0.10499359]]
</code></pre><p>Now,we use the implemented sigmoid function to create the graph to understand the behavior of this function:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">%matplotlib inline</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"></div><div class="line">x = np.arange(<span class="number">-10.</span>, <span class="number">10.</span>, <span class="number">0.2</span>)</div><div class="line">y = sigmoid(x)</div><div class="line">plt.plot(x,y)</div><div class="line">plt.show()</div></pre></td></tr></table></figure>
<p><img src="How-to-implement-a-neural-network/output_11_0.png" alt=""></p>
<p>From the above graph,we can observe that the sigmoid function produces the curve which will be in shape “S”,and returns the output value which falls in the range of 0 to 1.<br>The below are the properties of the Sigmoid function:</p>
<ul>
<li>The high value will have the high score but not the higher score.</li>
<li>Used for binary classification in logistic regression model.</li>
<li>The probabilities sum need not be 1.</li>
</ul>
<p>The below is the graph of the gradient of the sigmoid function:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">%matplotlib inline</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"></div><div class="line">x = np.arange(<span class="number">-10.</span>, <span class="number">10.</span>, <span class="number">0.2</span>)</div><div class="line">y = sigmoid_grad(x)</div><div class="line">plt.plot(x,y)</div><div class="line">plt.show()</div></pre></td></tr></table></figure>
<p><img src="How-to-implement-a-neural-network/output_13_0.png" alt=""></p>
<h3 id="To-make-debugging-easier-we-will-now-implement-a-gradient-checker-Fill-in-the-implementation-for-gradcheck-naive-in-q2-gradcheck-py"><a href="#To-make-debugging-easier-we-will-now-implement-a-gradient-checker-Fill-in-the-implementation-for-gradcheck-naive-in-q2-gradcheck-py" class="headerlink" title="To make debugging easier, we will now implement a gradient checker. Fill in the implementation for gradcheck naive in q2 gradcheck.py."></a>To make debugging easier, we will now implement a gradient checker. Fill in the implementation for gradcheck naive in q2 gradcheck.py.</h3><p><strong>Gradient checks</strong></p>
<p>In theory,performing a gradient check is as simple as comparing the analytic gradient and the numerical gradient.We use the <em>centered</em> difference formula of the form:</p>
<p><img src="How-to-implement-a-neural-network/gradcheck.png" alt=""></p>
<p>where ϵ is a very small number,in practice approximately <em>le-5</em> or so.<br><strong><em>Use relative for the comparison.</em></strong><br>It is always more appropriate to consider the <em>relative error</em>.</p>
<ul>
<li>relative error &gt; le-2 usually means the gradient is probably wrong.</li>
<li>le-7 and less you should be happy.</li>
</ul>
<p>That means,to implement the gradient check,we would need:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradcheck_naive</span><span class="params">(f, x)</span>:</span></div><div class="line">    <span class="string">""" Gradient check for a function f.</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    Arguments:</span></div><div class="line"><span class="string">    f -- a function that takes a single argument and outputs the</span></div><div class="line"><span class="string">         cost and its gradients</span></div><div class="line"><span class="string">    x -- the point (numpy array) to check the gradient at</span></div><div class="line"><span class="string">    """</span></div><div class="line">    rndstate = random.getstate()</div><div class="line">    random.setstate(rndstate)</div><div class="line">    fx, grad = f(x) <span class="comment"># Evaluate function value at original point</span></div><div class="line">    h = <span class="number">1e-4</span>        <span class="comment"># Do not change this!</span></div><div class="line">    <span class="comment"># Iterate over all indexes in x</span></div><div class="line">    it = np.nditer(x, flags=[<span class="string">'multi_index'</span>], op_flags=[<span class="string">'readwrite'</span>])</div><div class="line">    <span class="keyword">while</span> <span class="keyword">not</span> it.finished:</div><div class="line">        ix = it.multi_index</div><div class="line">        <span class="comment"># Try modifying x[ix] with h defined above to compute</span></div><div class="line">        <span class="comment"># numerical gradients. Make sure you call random.setstate(rndstate)</span></div><div class="line">        <span class="comment"># before calling f(x) each time. This will make it possible</span></div><div class="line">        <span class="comment"># to test cost functions with built in randomness later.</span></div><div class="line">        x[ix] += h</div><div class="line">        random.setstate(rndstate)</div><div class="line">        fhigh = f(x)[<span class="number">0</span>]</div><div class="line">        x[ix] -= <span class="number">2</span>*h</div><div class="line">        random.setstate(rndstate)</div><div class="line">        flow = f(x)[<span class="number">0</span>]</div><div class="line">        x[ix] += h</div><div class="line">        numgrad = ((fhigh-flow)/(<span class="number">2</span>*h)).sum()</div><div class="line">        <span class="comment">#raise NotImplementedError</span></div><div class="line">        <span class="comment"># Compare gradients</span></div><div class="line">        reldiff = abs(numgrad - grad[ix]) / max(<span class="number">1</span>, abs(numgrad), abs(grad[ix]))</div><div class="line">        <span class="keyword">if</span> reldiff &gt; <span class="number">1e-5</span>:</div><div class="line">            <span class="keyword">print</span> <span class="string">"Gradient check failed."</span></div><div class="line">            <span class="keyword">print</span> <span class="string">"First gradient error found at index %s"</span> % str(ix)</div><div class="line">            <span class="keyword">print</span> <span class="string">"Your gradient: %f \t Numerical gradient: %f"</span> % (</div><div class="line">                grad[ix], numgrad)</div><div class="line">            <span class="keyword">return</span></div><div class="line"></div><div class="line">        it.iternext() <span class="comment"># Step to next dimension</span></div><div class="line"></div><div class="line">    <span class="keyword">print</span> <span class="string">"Gradient check passed!"</span></div></pre></td></tr></table></figure>
<p>Then generate trial function that are used by multiple tests:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> random</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">sanity_check</span><span class="params">()</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line"><span class="string">    Some basic sanity checks.</span></div><div class="line"><span class="string">    """</span></div><div class="line">    quad = <span class="keyword">lambda</span> x: (np.sum(x ** <span class="number">2</span>), x * <span class="number">2</span>)</div><div class="line"></div><div class="line">    <span class="keyword">print</span> <span class="string">"Running sanity checks..."</span></div><div class="line">    gradcheck_naive(quad, np.array(<span class="number">123.456</span>))      <span class="comment"># scalar test</span></div><div class="line">    gradcheck_naive(quad, np.random.randn(<span class="number">3</span>,))    <span class="comment"># 1-D test</span></div><div class="line">    gradcheck_naive(quad, np.random.randn(<span class="number">4</span>,<span class="number">5</span>))   <span class="comment"># 2-D test</span></div><div class="line">    <span class="keyword">print</span> <span class="string">""</span></div><div class="line"></div><div class="line">sanity_check()</div></pre></td></tr></table></figure>
<pre><code>Running sanity checks...
Gradient check passed!
Gradient check passed!
Gradient check passed!
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">your_sanity_checks</span><span class="params">()</span>:</span></div><div class="line"></div><div class="line">    <span class="keyword">print</span> <span class="string">"Running your sanity checks..."</span></div><div class="line">    test1_function = <span class="keyword">lambda</span> x: (np.log(x), <span class="number">1</span>/x)</div><div class="line">    <span class="keyword">print</span> <span class="string">"The original function is: ln(x)"</span></div><div class="line">    gradcheck_naive(test1_function, np.array(<span class="number">123.456</span>))      <span class="comment"># scalar test</span></div><div class="line">    <span class="comment">#gradcheck_naive(test1_function, np.random.randn(3,))    # 1-D test</span></div><div class="line">    <span class="comment">#gradcheck_naive(test1_function, np.random.randn(4,5))   # 2-D test</span></div><div class="line"></div><div class="line">    test2_function = <span class="keyword">lambda</span> x: (np.sin(x)-np.cos(x), np.cos(x)+np.sin(x))</div><div class="line">    <span class="keyword">print</span> <span class="string">"The original function is: sin(x)-cos(x)"</span></div><div class="line">    gradcheck_naive(test2_function, np.array(<span class="number">123.456</span>))      <span class="comment"># scalar test</span></div><div class="line">    gradcheck_naive(test2_function, np.random.randn(<span class="number">3</span>,))    <span class="comment"># 1-D test</span></div><div class="line">    gradcheck_naive(test2_function, np.random.randn(<span class="number">4</span>,<span class="number">5</span>))   <span class="comment"># 2-D test</span></div><div class="line"></div><div class="line">    test3_function = <span class="keyword">lambda</span> x: (np.exp(x), np.exp(x))</div><div class="line">    <span class="keyword">print</span> <span class="string">"The original function is: exp(x)"</span></div><div class="line">    gradcheck_naive(test3_function, np.array(<span class="number">123.456</span>))      <span class="comment"># scalar test</span></div><div class="line">    gradcheck_naive(test3_function, np.random.randn(<span class="number">3</span>,))    <span class="comment"># 1-D test</span></div><div class="line">    gradcheck_naive(test3_function, np.random.randn(<span class="number">4</span>,<span class="number">5</span>))   <span class="comment"># 2-D test</span></div><div class="line">your_sanity_checks()</div></pre></td></tr></table></figure>
<pre><code>Running your sanity checks...
The original function is: ln(x)
Gradient check passed!
The original function is: sin(x)-cos(x)
Gradient check passed!
Gradient check passed!
Gradient check passed!
The original function is: exp(x)
Gradient check passed!
Gradient check passed!
Gradient check passed!
</code></pre><h3 id="implement-the-forward-and-backward-passes-for-a-neural-network-with-one-sigmoid-hidden-layer"><a href="#implement-the-forward-and-backward-passes-for-a-neural-network-with-one-sigmoid-hidden-layer" class="headerlink" title="implement the forward and backward passes for a neural network with one sigmoid hidden layer."></a>implement the forward and backward passes for a neural network with one sigmoid hidden layer.</h3><p>Let’s consider a simple logistic regression classifier.</p>
<p>The <strong><em>forward</em></strong> pass computes the output given the input for inference.In forward composes the computation of each layer to computation of each layer to compute the “function” represented by the model.</p>
<p><img src="How-to-implement-a-neural-network/forward.png" alt=""></p>
<p>The <strong><em>backward</em></strong> pass computes the gradient given the loss for learning.In backward composes the gradient of each layer to compute the gradient of the whole model by automatic differentiation.</p>
<p><img src="How-to-implement-a-neural-network/backward.png" alt=""></p>
<p>The below is my implementation:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward_backward_prop</span><span class="params">(data, labels, params, dimensions)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line"><span class="string">    Forward and backward propagation for a two-layer sigmoidal network</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    Compute the forward propagation and for the cross entropy cost,</span></div><div class="line"><span class="string">    and backward propagation for the gradients for all parameters.</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    Arguments:</span></div><div class="line"><span class="string">    data -- M x Dx matrix, where each row is a training example.</span></div><div class="line"><span class="string">    labels -- M x Dy matrix, where each row is a one-hot vector.</span></div><div class="line"><span class="string">    params -- Model parameters, these are unpacked for you.</span></div><div class="line"><span class="string">    dimensions -- A tuple of input dimension, number of hidden units</span></div><div class="line"><span class="string">                  and output dimension</span></div><div class="line"><span class="string">    """</span></div><div class="line">    <span class="comment">### Unpack network parameters (do not modify)</span></div><div class="line">    ofs = <span class="number">0</span></div><div class="line">    Dx, H, Dy = (dimensions[<span class="number">0</span>], dimensions[<span class="number">1</span>], dimensions[<span class="number">2</span>])</div><div class="line">    W1 = np.reshape(params[ofs:ofs+ Dx * H], (Dx, H))</div><div class="line">    ofs += Dx * H</div><div class="line">    b1 = np.reshape(params[ofs:ofs + H], (<span class="number">1</span>, H))</div><div class="line">    ofs += H</div><div class="line">    W2 = np.reshape(params[ofs:ofs + H * Dy], (H, Dy))</div><div class="line">    ofs += H * Dy</div><div class="line">    b2 = np.reshape(params[ofs:ofs + Dy], (<span class="number">1</span>, Dy))</div><div class="line">    </div><div class="line">    <span class="comment">### forward propagation</span></div><div class="line">    hidden_value = sigmoid(np.dot(data,W1)+b1)</div><div class="line">    y_value = softmax(np.dot(hidden_value,W2)+b2)</div><div class="line">    cost = np.sum(-np.log(y_value[labels == <span class="number">1</span>]))/data.shape[<span class="number">0</span>]</div><div class="line"></div><div class="line">    <span class="comment">### backward propagation</span></div><div class="line">    d_y = (y_value-labels)/data.shape[<span class="number">0</span>]</div><div class="line">    gradW2 = np.dot(hidden_value.T, d_y)</div><div class="line">    gradb2 = np.sum(d_y,axis=<span class="number">0</span>,keepdims=<span class="keyword">True</span>)</div><div class="line">    gradh = np.dot(d_y,W2.T)*sigmoid_grad(hidden_value)</div><div class="line">    gradW1 = np.dot(data.T, gradh)</div><div class="line">    gradb1 = np.sum(gradh,axis=<span class="number">0</span>,keepdims=<span class="keyword">True</span>)</div><div class="line"></div><div class="line">    <span class="comment">### Stack gradients (do not modify)</span></div><div class="line">    grad = np.concatenate((gradW1.flatten(), gradb1.flatten(),</div><div class="line">        gradW2.flatten(), gradb2.flatten()))</div><div class="line"></div><div class="line">    <span class="keyword">return</span> cost, grad</div></pre></td></tr></table></figure>
<p>Then generate trial function that is used by test:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn.utils.extmath <span class="keyword">import</span> softmax</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">sanity_check</span><span class="params">()</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line"><span class="string">    Set up fake data and parameters for the neural network, and test using</span></div><div class="line"><span class="string">    gradcheck.</span></div><div class="line"><span class="string">    """</span></div><div class="line">    <span class="keyword">print</span> <span class="string">"Running sanity check..."</span></div><div class="line"></div><div class="line">    N = <span class="number">20</span></div><div class="line">    dimensions = [<span class="number">10</span>, <span class="number">5</span>, <span class="number">10</span>]</div><div class="line">    data = np.random.randn(N, dimensions[<span class="number">0</span>])   <span class="comment"># each row will be a datum</span></div><div class="line">    labels = np.zeros((N, dimensions[<span class="number">2</span>]))</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> xrange(N):</div><div class="line">        labels[i, random.randint(<span class="number">0</span>,dimensions[<span class="number">2</span>]<span class="number">-1</span>)] = <span class="number">1</span></div><div class="line"></div><div class="line">    params = np.random.randn((dimensions[<span class="number">0</span>] + <span class="number">1</span>) * dimensions[<span class="number">1</span>] + (</div><div class="line">        dimensions[<span class="number">1</span>] + <span class="number">1</span>) * dimensions[<span class="number">2</span>], )</div><div class="line">    gradcheck_naive(<span class="keyword">lambda</span> params:</div><div class="line">        forward_backward_prop(data, labels, params, dimensions), params)</div><div class="line">sanity_check()</div></pre></td></tr></table></figure>
<pre><code>Running sanity check...
Gradient check passed!
</code></pre>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://ZzzBe.github.io/How-to-understand-softmax-function/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="ZHANG Bo">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jack's Notebook">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/How-to-understand-softmax-function/" itemprop="url">How to understand softmax function</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-09-19T04:43:37+08:00">
                2017-09-19
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p> <strong><em>Softmax</em></strong></p>
<h2 id="The-property-of-softmax-function"><a href="#The-property-of-softmax-function" class="headerlink" title="The property of softmax function"></a>The property of softmax function</h2><p>(a) prove that <strong><em>softmax</em></strong> is invariant to constant offsets in the input,that is, for any input vecotr <strong><em>x</em></strong> and any constant <strong><em>c</em></strong>,<br>$$softmax(x + c) = softmax(x)$$<br>which <strong><em>x+c</em></strong> means adding the constant <strong><em>c</em></strong> to every dimension of <strong><em>x</em></strong>.</p>
<p>First all of,to prove the <strong><em>softmax</em></strong> function:</p>
<p>$$softmax(x_i + c) = \frac {e^{x_i + c}}{\sum_j e^{x_i + c}} $$<br>$$ = \frac {e^x_i \cdot e^c}{\sum_j e^{x_i} \cdot e^c} $$<br>$$ = \frac {e^x_i}{\sum_j e^{x_i}} $$<br>$$ = softmax(x_i) $$</p>
<h2 id="Implementation-in-python"><a href="#Implementation-in-python" class="headerlink" title="Implementation in python."></a>Implementation in python.</h2><p>(b)Write my implementation in <strong><em>q1_softmax.py</em></strong> and test it.</p>
<p>From the mathematical point of view,let’s try my solution(<strong><em>Jacksoftmax</em></strong>):</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">Jacksoftmax</span><span class="params">(x)</span>:</span></div><div class="line">    <span class="string">"""Compute the softmax function for each row of the input x.</span></div><div class="line"><span class="string">    Arguments:</span></div><div class="line"><span class="string">    x -- A N dimensional vector or M x N dimensional numpy matrix.</span></div><div class="line"><span class="string">    Return:</span></div><div class="line"><span class="string">    x -- You are allowed to modify x in-place</span></div><div class="line"><span class="string">    """</span></div><div class="line">    orig_shape = x.shape</div><div class="line">    <span class="keyword">if</span> len(x.shape) &gt; <span class="number">1</span>:</div><div class="line">        <span class="comment"># Matrix</span></div><div class="line">        max_row = np.max(x, axis=<span class="number">1</span>)</div><div class="line">        e_x = np.exp(x - max_row[:,np.newaxis])</div><div class="line">        div = np.sum(e_x, axis=<span class="number">1</span>)</div><div class="line">        x = e_x/div[:,np.newaxis]</div><div class="line">        <span class="comment">#raise NotImplementedError</span></div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        <span class="comment"># Vector</span></div><div class="line">        x = np.exp((x-x.min())/(x.max()-x.min()))</div><div class="line">        sum_column = sum(x)</div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(x.size):</div><div class="line">            x[i] = x[i]/float(sum_column)</div><div class="line">        <span class="comment">#raise NotImplementedError</span></div><div class="line">    <span class="keyword">assert</span> x.shape == orig_shape</div><div class="line">    <span class="keyword">return</span> x</div></pre></td></tr></table></figure>
<p>Let’s take basic test example:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn.utils.extmath <span class="keyword">import</span> softmax</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_softmax_basic</span><span class="params">()</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line"><span class="string">    Some simple tests to get you started.</span></div><div class="line"><span class="string">    Warning: these are not exhaustive.</span></div><div class="line"><span class="string">    """</span></div><div class="line">    <span class="keyword">print</span> <span class="string">"This is the out:"</span></div><div class="line">    test1 = Jacksoftmax(np.array([<span class="number">1</span>,<span class="number">2</span>]))</div><div class="line">    <span class="keyword">print</span> test1</div><div class="line">    ans1 = np.array([<span class="number">0.26894142</span>,  <span class="number">0.73105858</span>])</div><div class="line">    <span class="keyword">assert</span> np.allclose(test1, ans1, rtol=<span class="number">1e-05</span>, atol=<span class="number">1e-06</span>)</div><div class="line"></div><div class="line">    test2 = Jacksoftmax(np.array([[<span class="number">1001</span>,<span class="number">1002</span>],[<span class="number">3</span>,<span class="number">4</span>]]))</div><div class="line">    <span class="keyword">print</span> test2</div><div class="line">    ans2 = np.array([</div><div class="line">        [<span class="number">0.26894142</span>, <span class="number">0.73105858</span>],</div><div class="line">        [<span class="number">0.26894142</span>, <span class="number">0.73105858</span>]])</div><div class="line">    <span class="keyword">assert</span> np.allclose(test2, ans2, rtol=<span class="number">1e-05</span>, atol=<span class="number">1e-06</span>)</div><div class="line"></div><div class="line">    test3 = Jacksoftmax(np.array([[<span class="number">-1001</span>,<span class="number">-1002</span>]]))</div><div class="line">    <span class="keyword">print</span> test3</div><div class="line">    ans3 = np.array([<span class="number">0.73105858</span>, <span class="number">0.26894142</span>])</div><div class="line">    <span class="keyword">assert</span> np.allclose(test3, ans3, rtol=<span class="number">1e-05</span>, atol=<span class="number">1e-06</span>)</div><div class="line">    </div><div class="line">test_softmax_basic()</div></pre></td></tr></table></figure>
<pre><code>This is the out:
[ 0.26894142  0.73105858]
[[ 0.26894142  0.73105858]
 [ 0.26894142  0.73105858]]
[[ 0.73105858  0.26894142]]
</code></pre><p>Use this space to test your softmax implementation by running:<br>        python q1_softmax.py</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_softmax</span><span class="params">()</span>:</span></div><div class="line">    <span class="keyword">print</span> <span class="string">"Running your tests..."</span></div><div class="line">    <span class="keyword">print</span> <span class="string">"Enter the dimension of the matrix:"</span></div><div class="line">    n = input()</div><div class="line">    m = input()</div><div class="line">    test1 = np.random.random((n,m))</div><div class="line">    <span class="keyword">print</span> <span class="string">"The first sample is:"</span></div><div class="line">    <span class="keyword">print</span> test1</div><div class="line">    sk_array1 = softmax(test1)</div><div class="line">    <span class="keyword">print</span> <span class="string">"The answer with softmax of sklearn:"</span></div><div class="line">    <span class="keyword">print</span> sk_array1</div><div class="line">    jk_array1 = Jacksoftmax(test1)</div><div class="line">    <span class="keyword">print</span> <span class="string">"The answer with my softmax:"</span></div><div class="line">    <span class="keyword">print</span> jk_array1</div><div class="line">    <span class="keyword">assert</span> np.allclose(sk_array1, jk_array1, rtol=<span class="number">1e-05</span>, atol=<span class="number">1e-06</span>)</div><div class="line">    test2 = <span class="number">2.5</span> * np.random.randn(n,m)+<span class="number">3</span></div><div class="line">    <span class="keyword">print</span> <span class="string">"The second sample is:"</span></div><div class="line">    <span class="keyword">print</span> test2</div><div class="line">    sk_array2 = softmax(test2)</div><div class="line">    <span class="keyword">print</span> <span class="string">"The answer with softmax of sklearn:"</span></div><div class="line">    <span class="keyword">print</span> sk_array2</div><div class="line">    jk_array2 = Jacksoftmax(test2)</div><div class="line">    <span class="keyword">print</span> <span class="string">"The answer with my softmax:"</span></div><div class="line">    <span class="keyword">print</span> jk_array2</div><div class="line">    <span class="keyword">assert</span> np.allclose(sk_array2, jk_array2, rtol=<span class="number">1e-05</span>, atol=<span class="number">1e-06</span>)</div><div class="line">test_softmax()</div></pre></td></tr></table></figure>
<pre><code>Running your tests...
Enter the dimension of the matrix:
3
5
The first sample is:
[[ 0.0345306   0.18920816  0.49485403  0.03642509  0.90002219]
 [ 0.9100086   0.61635177  0.8217402   0.69910275  0.59564619]
 [ 0.01915225  0.8879781   0.35720632  0.57615048  0.09329815]]
The answer with softmax of sklearn:
[[ 0.14025367  0.16371552  0.22224406  0.14051963  0.33326711]
 [ 0.23802995  0.17745901  0.21792002  0.19276863  0.1738224 ]
 [ 0.13142823  0.31333935  0.18429081  0.22939815  0.14154346]]
The answer with my softmax:
[[ 0.14025367  0.16371552  0.22224406  0.14051963  0.33326711]
 [ 0.23802995  0.17745901  0.21792002  0.19276863  0.1738224 ]
 [ 0.13142823  0.31333935  0.18429081  0.22939815  0.14154346]]
The second sample is:
[[ 4.43086839  4.35889742 -1.00466532  0.65716001  6.19049139]
 [ 5.66895751  3.98764183  5.77718425 -0.33103714 -0.18460528]
 [-1.35308692  1.1158278  -0.9865306   3.25704449  0.23047689]]
The answer with softmax of sklearn:
[[  1.28731109e-01   1.19791752e-01   5.61127060e-04   2.95653336e-03
    7.47959479e-01]
 [  4.33693148e-01   8.07228412e-02   4.83264437e-01   1.07502359e-03
    1.24455056e-03]
 [  8.35966458e-03   9.87244880e-02   1.20609700e-02   8.40124177e-01
    4.07307003e-02]]
The answer with my softmax:
[[  1.28731109e-01   1.19791752e-01   5.61127060e-04   2.95653336e-03
    7.47959479e-01]
 [  4.33693148e-01   8.07228412e-02   4.83264437e-01   1.07502359e-03
    1.24455056e-03]
 [  8.35966458e-03   9.87244880e-02   1.20609700e-02   8.40124177e-01
    4.07307003e-02]]
</code></pre><p>Follow the result i will get the correct answer by doing vectorization.</p>
<h2 id="How-to-use-it"><a href="#How-to-use-it" class="headerlink" title="How to use it?"></a>How to use it?</h2><p>Now,We use the implemented softmax function to create the graph to understand the behavior of this function:</p>
<ul>
<li>To create a list which contains values in the range of 0 to 10</li>
<li>Next,pass the list to calculate the scores from the softmax function</li>
<li>To create a graph.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"></div><div class="line">%matplotlib inline</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmax_graph</span><span class="params">(x, y, x_title, y_title)</span>:</span></div><div class="line">    plt.plot(x, y)</div><div class="line">    plt.xlabel(x_title)</div><div class="line">    plt.ylabel(y_title)</div><div class="line">    plt.show()</div><div class="line"></div><div class="line"></div><div class="line">x = np.arange(<span class="number">0</span>,<span class="number">30</span>)</div><div class="line">y = Jacksoftmax(x)</div><div class="line"><span class="keyword">print</span> x</div><div class="line"><span class="keyword">print</span> y</div><div class="line">softmax_graph(x, y, <span class="string">"Input"</span>, <span class="string">"Softmax Scores"</span>)</div></pre></td></tr></table></figure>
<pre><code>[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24
 25 26 27 28 29]
[ 0.03152756  0.03152756  0.03152756  0.03152756  0.03152756  0.03152756
  0.03152756  0.03152756  0.03152756  0.03152756  0.03152756  0.03152756
  0.03152756  0.03152756  0.03152756  0.03152756  0.03152756  0.03152756
  0.03152756  0.03152756  0.03152756  0.03152756  0.03152756  0.03152756
  0.03152756  0.03152756  0.03152756  0.03152756  0.03152756  0.08570079]
</code></pre><p><img src="How-to-understand-softmax-function/output_9_1.png" alt=""></p>
<p>The figure shows the property of softmax function:<br><strong><em> The high value gets the hign score(means probability).</em></strong></p>
<p>We can use the softmax function to multi-classification task.</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  


          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">ZHANG Bo</p>
              <p class="site-description motion-element" itemprop="description">Stay hungry.Stay foolish.</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">3</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            

            
              
              
              <div class="site-state-item site-state-tags">
                
                  <span class="site-state-item-count">2</span>
                  <span class="site-state-item-name">tags</span>
                
              </div>
            

          </nav>

          

          <div class="links-of-author motion-element">
            
          </div>

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">ZHANG Bo</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a> v5.1.3</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.3"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.3"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.3"></script>



  


  




	





  





  












  





  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  


  <!-- 背景动画 -->
  <script type="text/javascript" src="/js/src/particle.js"></script>
</body>
</html>
